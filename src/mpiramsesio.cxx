/*! \file mpiramsesio.cxx
 *  \brief this file contains routines used with MPI compilation and ramses io and domain construction.
 */

#ifdef USEMPI

//-- For MPI

#include "stf.h"

#include "ramsesitems.h"
#include "endianutils.h"

/// \name RAMSES Domain decomposition
//@{

/*!
    Determine the domain decomposition.\n
    Here the domains are constructured in data units
    only ThisTask==0 should call this routine. It is tricky to get appropriate load balancing and correct number of particles per processor.\n

    I could use recursive binary splitting like kd-tree along most spread axis till have appropriate number of volumes corresponding
    to number of processors.

    NOTE: assume that cannot store data so position information is read Nsplit times to determine boundaries of subvolumes
    could also randomly subsample system and produce tree from that
    should store for each processor the node structure generated by the domain decomposition
    what I could do is read file twice, one to get extent and other to calculate entropy then decompose
    along some primary axis, then choose orthogonal axis, keep iterating till have appropriate number of subvolumes
    then store the boundaries of the subvolume. this means I don't store data but get at least reasonable domain decomposition

    NOTE: pkdgrav uses orthoganal recursive bisection along with kd-tree, gadget-2 uses peno-hilbert curve to map particles and oct-trees
    the question with either method is guaranteeing load balancing. for ORB achieved by splitting (sub)volume along a dimension (say one with largest spread or max entropy)
    such that either side of the cut has approximately the same number of particles (ie: median splitting). But for both cases, load balancing requires particle information
    so I must load the system then move particles about to ensure load balancing.

    Main thing first is get the dimensional extent of the system.
    then I could get initial splitting just using mid point between boundaries along each dimension.
    once have that initial splitting just load data then start shifting data around.
*/
void MPIDomainExtentRAMSES(Options &opt){
    Int_t i;
    char buf[2000];
    fstream Framses;
    RAMSES_Header ramses_header_info;
    string stringbuf;
    if (ThisTask==0) {
    sprintf(buf,"%s/info_%s.txt",opt.fname,opt.ramsessnapname);
    Framses.open(buf, ios::in);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    getline(Framses,stringbuf);
    Framses>>stringbuf>>stringbuf>>ramses_header_info.BoxSize;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.time;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.aexp;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.HubbleParam;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.Omegam;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.OmegaLambda;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.Omegak;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.Omegab;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.scale_l;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.scale_d;
    Framses>>stringbuf>>stringbuf>>ramses_header_info.scale_t;
    Framses.close();
    ///note that code units are 0 to 1
    for (int j=0;j<3;j++) {mpi_xlim[j][0]=0;mpi_xlim[j][1]=1.0;}

    //There may be issues with particles exactly on the edge of a domain so before expanded limits by a small amount
    //now only done if a specific compile option passed
#ifdef MPIEXPANDLIM
    for (int j=0;j<3;j++) {
        Double_t dx=0.001*(mpi_xlim[j][1]-mpi_xlim[j][0]);
        mpi_xlim[j][0]-=dx;mpi_xlim[j][1]+=dx;
    }
#endif
    }

    //make sure limits have been found
    MPI_Barrier(MPI_COMM_WORLD);
    if (NProcs==1) {
        for (i=0;i<3;i++) {
            mpi_domain[ThisTask].bnd[i][0]=mpi_xlim[i][0];
            mpi_domain[ThisTask].bnd[i][1]=mpi_xlim[i][1];
        }
    }
}

void MPIDomainDecompositionRAMSES(Options &opt){
#define SKIP2 Fgad[i].read((char*)&dummy, sizeof(dummy));
    Int_t i,j,k,n,m;
    int Nsplit,isplit;

    if (ThisTask==0) {
    }
}

///reads a ramses file to determine number of particles in each MPIDomain
///\todo need to add code to read gas cell positions and send them to the appropriate mpi thead
void MPINumInDomainRAMSES(Options &opt)
{
    if (NProcs > 1)
    {
        MPIDomainExtentRAMSES(opt);
        MPIInitialDomainDecomposition();
        MPIDomainDecompositionRAMSES(opt);
        //Int_t i,j,k,n,m,temp,Ntot,indark,ingas,instar;
	int i,j,k,n,m,temp,Ntot,indark,ingas,instar;
        int idim,ivar,igrid;
        Int_t idval;
        int   typeval;
        Int_t Nlocalold=Nlocal;
        RAMSESFLOAT xtemp[3], ageval;
        Double_t mtemp;
        MPI_Status status;
        Int_t Nlocalbuf,ibuf=0,*Nbuf, *Nbaryonbuf;
        int *ngridlevel,*ngridbound,*ngridfile;
        int lmin=1000000,lmax=0;

        char buf[2000],buf1[2000],buf2[2000];
        string stringbuf,orderingstring;
        fstream Finfo;
        fstream *Fpart, *Famr, *Fhydro;
        fstream  Framses;
        RAMSES_Header *header;
        int intbuff[NRAMSESTYPE];
        long long longbuff[NRAMSESTYPE];
	int ncpu, nboundary;
	int *ncache, *numbl, *numbb;
        Int_t count2,bcount2;
        int dummy,byteoffset;
        Int_t chunksize = opt.inputbufsize, nchunk;
        RAMSESFLOAT *xtempchunk, *mtempchunk, *agetempchunk;
        int *icellchunk;
	char *typechunk;
	double dx;
        Fpart      = new fstream[opt.num_files];
        Famr       = new fstream[opt.num_files];
        Fhydro     = new fstream[opt.num_files];
        header     = new RAMSES_Header[opt.num_files];
        double dmp_mass,OmegaM, OmegaB;
        int n_out_of_bounds = 0;
        int ndark  = 0;
        int nstar  = 0;
        int nghost = 0;
        int *ireadfile,*ireadtask,*readtaskID;
        ireadtask=new int[NProcs];
        readtaskID=new int[opt.nsnapread];
        ireadfile=new int[opt.num_files];
        MPIDistributeReadTasks(opt,ireadtask,readtaskID);
        MPISetFilesRead(opt,ireadfile,ireadtask);

        Nbuf=new Int_t[NProcs];
        Nbaryonbuf=new Int_t[NProcs];
        for (int j=0;j<NProcs;j++) Nbuf[j]=0;
        for (int j=0;j<NProcs;j++) Nbaryonbuf[j]=0;

	vector<string> partfields;
	string fieldname;
	int fieldlength, numfields;
        if (ThisTask == 0) {
	    // List existing particle fields from part_file_descriptor.txt
	    sprintf(buf1,"%s/part_file_descriptor.txt", opt.fname);
	    partfields = RAMSES_read_descriptor(buf1);
	    numfields = partfields.size();
        }
	MPI_Bcast(&numfields, 1, MPI_INT, 0, MPI_COMM_WORLD);

	for (j=0; j<numfields; ++j) {
	    // Retrieve and broadcast the length of the string
	    if (ThisTask == 0) {
		fieldname = partfields[j];
		fieldlength = partfields[j].size();
	    }
	    MPI_Bcast(&fieldlength, 1, MPI_INT, 0, MPI_COMM_WORLD);
	    // Create a string with the right size
	    if (ThisTask > 0) fieldname.resize(fieldlength);
	    // Finally broadcast the string
	    MPI_Bcast(const_cast<char*>(fieldname.data()), fieldlength, MPI_CHAR, 0, MPI_COMM_WORLD);
	    if (ThisTask > 0) partfields.push_back(fieldname);
	}




        if (ireadtask[ThisTask]>=0) {
            if (opt.partsearchtype!=PSTGAS && opt.partsearchtype!=PSTBH) {
                for (i = 0, count2 = 0; i < opt.num_files; i++) if (ireadfile[i]){
                    sprintf(buf1,"%s/part_%s.out%05d",opt.fname,opt.ramsessnapname,i+1);
                    sprintf(buf2,"%s/part_%s.out",opt.fname,opt.ramsessnapname);
                    if (FileExists(buf1)) sprintf(buf,"%s",buf1);
                    else if (FileExists(buf2)) sprintf(buf,"%s",buf2);
                    Fpart[i].open      (buf, ios::binary|ios::in);
                    //skip header information in each file save for number in the file
                    //@{
                    byteoffset = 0;
                    // ncpus
                    byteoffset += RAMSES_fortran_skip(Fpart[i], 1);
                    // ndims
                    byteoffset += RAMSES_fortran_read(Fpart[i],header[i].ndim);
                    //store number of particles locally in file
                    byteoffset += RAMSES_fortran_read(Fpart[i],header[i].npartlocal);
                    // skip local seeds, nstartot, mstartot, mstarlost, nsink
                    byteoffset += RAMSES_fortran_skip(Fpart[i], 5);
                    // byteoffset now stores size of header offset for particles

                    //data loaded into memory in chunks
                    chunksize    = nchunk = header[i].npartlocal;
                    xtempchunk   = new RAMSESFLOAT  [3*chunksize];
		    typechunk    = new char         [chunksize];

		    for (j=0;j<partfields.size();++j)
		    {
			// The order should not be important, since we skip the fields we do not read
			if      (partfields[j] == "position_x")  {RAMSES_fortran_read(Fpart[i],&xtempchunk[0*nchunk]);}
			else if (partfields[j] == "position_y")  {RAMSES_fortran_read(Fpart[i],&xtempchunk[1*nchunk]);}
			else if (partfields[j] == "position_z")  {RAMSES_fortran_read(Fpart[i],&xtempchunk[2*nchunk]);}
			else if (partfields[j] == "family")      {RAMSES_fortran_read(Fpart[i],typechunk)            ;}
			else {RAMSES_fortran_skip(Fpart[i]);}
		    }


                    for (int nn = 0; nn < nchunk; nn++)
                    {
                        //this should be a ghost star particle (or a cloud, or whatever)
                        if ((typechunk[nn] != 1) && (typechunk[nn] != 2)) nghost++;
                        else
                        {
                            xtemp[0] = xtempchunk[nn];
                            xtemp[1] = xtempchunk[nn+nchunk];
                            xtemp[2] = xtempchunk[nn+2*nchunk];

                            if (typechunk[nn] == 1)
                            {
                                typeval = DARKTYPE;
                                ndark++;
                            }
                            else
                            {
                                typeval = STARTYPE;
                                nstar++;
                            }

                            //determine processor this particle belongs on based on its spatial position
                            ibuf = MPIGetParticlesProcessor(xtemp[0],xtemp[1],xtemp[2]);
                            /// Count total number of DM particles, Baryons, etc
                            //@{
                            if (opt.partsearchtype == PSTALL)
                            {
                            Nbuf[ibuf]++;
                            count2++;
                            }
                            else
                            {
                                if (opt.partsearchtype == PSTDARK)
                                {
                                    if (typeval == DARKTYPE)
                                    {
                                        Nbuf[ibuf]++;
                                        count2++;
                                    }
                                    else
                                    {
                                        if (opt.iBaryonSearch)
                                        {
                                            Nbaryonbuf[ibuf]++;
                                        }
                                    }
                                }
                                else
                                {
                                    if (opt.partsearchtype == PSTSTAR)
                                    {
                                        if (typeval == STARTYPE)
                                        {
                                            Nbuf[ibuf]++;
                                            count2++;
                                        }
                                    }
                                }
                            }
                        }
                    }
                    delete[] xtempchunk;
                    delete[] typechunk;

                    Fpart[i].close();
                }
            }

            // now process gas if necessary
            if (opt.partsearchtype==PSTGAS || opt.partsearchtype==PSTALL || (opt.partsearchtype==PSTDARK&&opt.iBaryonSearch)) {
                for (i=0;i<opt.num_files;i++) if (ireadfile[i]) {
                    sprintf(buf1,"%s/amr_%s.out%05d",opt.fname,opt.ramsessnapname,i+1);
                    sprintf(buf2,"%s/amr_%s.out",opt.fname,opt.ramsessnapname);
                    if (FileExists(buf1)) sprintf(buf,"%s",buf1);
                    else if (FileExists(buf2)) sprintf(buf,"%s",buf2);
                    Famr[i].open(buf, ios::binary|ios::in);
                    sprintf(buf1,"%s/hydro_%s.out%05d",opt.fname,opt.ramsessnapname,i+1);
                    sprintf(buf2,"%s/hydro_%s.out",opt.fname,opt.ramsessnapname);
                    if (FileExists(buf1)) sprintf(buf,"%s",buf1);
                    else if (FileExists(buf2)) sprintf(buf,"%s",buf2);
                    Fhydro[i].open(buf, ios::binary|ios::in);
                    //read some of the amr header till get to number of cells in current file
                    //@{
                    byteoffset=0;
		    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].nfiles);
                    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].ndim);
                    header[i].twotondim=pow(2,header[i].ndim);
                    Famr[i].read((char*)&dummy, sizeof(dummy));
                    Famr[i].read((char*)&header[i].nx, sizeof(int));
                    Famr[i].read((char*)&header[i].ny, sizeof(int));
                    Famr[i].read((char*)&header[i].nz, sizeof(int));
                    Famr[i].read((char*)&dummy, sizeof(dummy));
                    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].nlevelmax);
                    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].ngridmax);
                    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].nboundary);
                    byteoffset+=RAMSES_fortran_read(Famr[i],header[i].npart[RAMSESGASTYPE]);

                    //then skip the rest (until taill included)
                    for (j=0;j<14;j++) RAMSES_fortran_skip(Famr[i]);
                    if (lmin>header[i].nlevelmax) lmin=header[i].nlevelmax;
                    if (lmax<header[i].nlevelmax) lmax=header[i].nlevelmax;
                    //@}
                    //read header info from hydro files
                    //@{
                    RAMSES_fortran_skip(Fhydro[i]);
                    RAMSES_fortran_read(Fhydro[i],header[i].nvarh);
                    RAMSES_fortran_skip(Fhydro[i]);
                    RAMSES_fortran_skip(Fhydro[i]);
                    RAMSES_fortran_skip(Fhydro[i]);
                    RAMSES_fortran_read(Fhydro[i],header[i].gamma_index);
                    //@}


		    ncpu = header[i].nfiles;
		    nboundary = header[i].nboundary;
		    // First, deal with AMR files
		    ncache=new int[(ncpu+nboundary)*header[i].nlevelmax];
		    // ncache should contains numbl and numbb

		    numbl=new int[ncpu*header[i].nlevelmax];
		    RAMSES_fortran_read(Famr[i],numbl);
			
		    // Fill the first part of ncache (ibound <= ncpu)
		    for (j=0;j<header[i].nlevelmax;j++) {
			for (k=0;k<ncpu;k++) {
			    ncache[(ncpu+nboundary)*j + k] = numbl[ncpu*j+k];
			}
		    }
			
		    // skip total number of grids per level: numbtot(1:10, 1:nlevelmax)
		    RAMSES_fortran_skip(Famr[i]);
		    if (header[i].nboundary>0) {
			// simple_boundary case, non periodic
			// skip headb, tailb
			RAMSES_fortran_skip(Famr[i], 2);
			// read numbb
			numbb=new int[header[i].nboundary*header[i].nlevelmax];
			RAMSES_fortran_read(Famr[i],numbb);
			// If needed, fill the rest of ncache
			for (j=0;j<header[i].nlevelmax;j++) {
			    for (k=0;k<nboundary;k++) {
				ncache[(ncpu+nboundary)*j + k+ncpu] = numbl[nboundary*j+k];
			    }
			}
		    }
		    // skip free memory (headf,tailf,numbf,used_mem,used_mem_tot) and ordering
		    RAMSES_fortran_skip(Famr[i], 2);
		    // skip keys
		    if (orderingstring==string("bisection")) RAMSES_fortran_skip(Famr[i],5);
		    else RAMSES_fortran_skip(Famr[i]);
		    // skip coarse levels (son, flag1, cpu_map)
		    RAMSES_fortran_skip(Famr[i], 3);

		    // TODO: also read hydro if we want to remove non-zoom regions
		    // start loop on levels for hydro and AMR
		    for (j=0;j<header[i].nlevelmax;j++) {
			dx = pow(0.5, j+1); // local cell size
			// start loop on boundaries for both
			for (k=0;k<(ncpu+nboundary);k++) {
			    // get ncache
			    chunksize = ncache[(ncpu+nboundary)*j + k];
			    if (chunksize>0) {
				// We want the cell positions from the AMR files
				// Skip ind_grid, next and prev
				RAMSES_fortran_skip(Famr[i], 3);
				// Store grid centres
				xtempchunk=new RAMSESFLOAT[3*chunksize];
				for (idim=0;idim<header[i].ndim;idim++) {
				    RAMSES_fortran_read(Famr[i],&xtempchunk[idim*chunksize]);
				}
				// Skip father (1) and nbor (2*ndim)
				RAMSES_fortran_skip(Famr[i], 1+2*header[i].ndim);
				// Read son index (2**ndim), needed to identify leaf cells
				icellchunk=new int[header[i].twotondim*chunksize];
				for (idim=0;idim<header[i].twotondim;idim++) {
				    RAMSES_fortran_read(Famr[i],&icellchunk[idim*chunksize]);
				}
				//skip cpu map and refinement map (2**ndim * 2)
				RAMSES_fortran_skip(Famr[i],2*header[i].twotondim);
			    } // chunksize > 0

			    if (chunksize>0) {
				for (idim=0;idim<header[i].twotondim;idim++) {  // loop over cells in octs (per dimension)
				    int ix=0, iy=0, iz=0;
				    for (igrid=0;igrid<chunksize;igrid++) {  // loop over cells in the chunk
					// Select only leaf cells (internal cells or at maximum level)
					if (icellchunk[idim*chunksize+igrid]==0 || j==header[i].nlevelmax-1) {
					    // Deal with positions
					    iz = idim/4;                  // z-position of the cell in the oct
					    iy = (idim - (4*iz))/2;       // y-position of the cell in the oct
					    ix = idim - (2*iy) - (4*iz);  // x-position of the cell in the oct
					    //  pos  = [ --------------- jitter -------------- ] + [ ------ grid centre ------ ] + [ position in oct ]
					    xtemp[0] = ( (float)rand()/(float)RAND_MAX - 0.5)*dx + xtempchunk[igrid+0*chunksize] + (double(ix)-0.5)*dx ;
					    xtemp[1] = ( (float)rand()/(float)RAND_MAX - 0.5)*dx + xtempchunk[igrid+1*chunksize] + (double(iy)-0.5)*dx ;
					    xtemp[2] = ( (float)rand()/(float)RAND_MAX - 0.5)*dx + xtempchunk[igrid+2*chunksize] + (double(iz)-0.5)*dx ;

					    //determine processor this particle belongs on based on its spatial position
					    ibuf=MPIGetParticlesProcessor(xtemp[0],xtemp[1],xtemp[2]);
					    if (opt.partsearchtype==PSTGAS || opt.partsearchtype==PSTALL) {
						// We look for gas or "everything"
						Nbuf[ibuf]++;
					    }
					    else {
						// We look for DM but still count baryons
						Nbaryonbuf[ibuf]++;
					    }
					}
				    }
				}
			    }

			    if (chunksize>0) {
				delete[] xtempchunk;
				delete[] icellchunk;
			    }
			}
		    }
		    Famr[i].close();
		    Fhydro[i].close();
		    }
	    }
	}
	//now having read number of particles, run all gather
        Int_t mpi_nlocal[NProcs];
        MPI_Allreduce(Nbuf,mpi_nlocal,NProcs,MPI_Int_t,MPI_SUM,MPI_COMM_WORLD);
        Nlocal=mpi_nlocal[ThisTask];
        if (opt.iBaryonSearch) {
            MPI_Allreduce(Nbaryonbuf,mpi_nlocal,NProcs,MPI_Int_t,MPI_SUM,MPI_COMM_WORLD);
            Nlocalbaryon[0]=mpi_nlocal[ThisTask];
        }
    }
}

//@}

#endif
